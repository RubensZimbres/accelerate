# AUTOGENERATED! DO NOT EDIT! File to edit: . (unless otherwise specified).

__all__ = []

# Cell
import torch
from collections import defaultdict
from fastcore.basics import patch
from fastai.learner import AvgSmoothLoss, Learner
from fastai.optimizer import Optimizer, OptimWrapper, _convert_params, pytorch_hp_map, _update, detuplify_pg
from fastai.torch_core import to_device, default_device, to_detach

# Cell
@patch
def accumulate(self:AvgSmoothLoss, learn):
    self.count += 1
    self.val = torch.lerp(to_detach(learn.loss.mean(), gather=True), self.val, self.beta)

# Cell
@patch
def step(self:Optimizer, closure=None):
    for p,pg,state,hyper in self.all_params(with_grad=True):
        for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))
        self.state[p] = state

# Cell
@patch
def __init__(self:OptimWrapper, params, opt, hp_map=None, convert_groups=True, **kwargs):
    if callable(opt):
        self.opt = opt(_convert_params(params), **kwargs) if convert_groups else opt(params, **kwargs)
    else:
        self.opt = opt
    if hp_map is None: hp_map = pytorch_hp_map
    self.fwd_map = {k: hp_map[k] if k in hp_map else k for k in detuplify_pg(self.opt.param_groups[0]).keys()}
    self.bwd_map = {v:k for k,v in self.fwd_map.items()}
    self.state = defaultdict(dict, {})
    self.frozen_idx = 0