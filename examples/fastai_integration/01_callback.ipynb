{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4408b579-d485-47b0-a0d3-2e74d7f4ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp accelerate_fastai_integration.callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792ba77-ef50-4782-aa75-fc12ae73a4b7",
   "metadata": {},
   "source": [
    "# Accelerate Callback\n",
    "> Support for using Hugging Face's Accelerate in `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717a8e1b-58d6-47c6-892a-0d46622615cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import store_attr, patch\n",
    "from fastai.callback.core import Callback, CancelBackwardException\n",
    "from fastai.distributed import DistributedDL\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import OptimWrapper\n",
    "from fastai.torch_core import to_device, default_device\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1149ac7-c9c0-4681-9482-b2a5fdeef4d9",
   "metadata": {},
   "source": [
    "## Accelerate\n",
    "\n",
    "[accelerate](https://huggingface.co/docs/accelerate/index) is a lightweight framework designed to handle device placement, dataloader configuration, and optimizer/schedulers so that the same code can work with a single GPU, multiple GPU's, and even TPU's. \n",
    "\n",
    "To use this integration, make sure accelerate is installed with:\n",
    "```bash\n",
    "pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b52a1ea-b980-42b4-9717-c0d2db17c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def _set_device(self:Learner, b):\n",
    "    if hasattr(self, \"accelerator\"): return to_device(b, self.accelerator.device)\n",
    "    else:\n",
    "        model_device = torch.device(torch.cuda.current_device()) if next(self.model.parameters()).is_cuda else torch.device(\"cpu\")\n",
    "        dls_device = getattr(self.dls, 'device', default_device())\n",
    "        if model_device == dls_device: return to_device(b, dls_device)\n",
    "        else: return to_device(b, model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b800e2-d79a-4da9-a544-e46ccfbc0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AcceleratorCallback(Callback):\n",
    "    \"A Callback that handles model, dataloader, and optimizer capabilities in distributed systems with Accelerate. Accepts Accelerator configuration parameters.\"\n",
    "    @delegates(Accelerator)\n",
    "    def __init__(self, **kwargs): self.accelerator = Accelerator(**kwargs)\n",
    "    \n",
    "    def before_fit(self):\n",
    "        \"Tie `self.accelerator` to the learner and prepare the model and optimizer\"\n",
    "        self.learn.accelerator = self.accelerator\n",
    "        self.learn.model = self.accelerator.prepare(self.learn.model)\n",
    "        self.prepare_optimizer()\n",
    "        \n",
    "    def prepare_optimizer(self):\n",
    "        \"Prepares the optimizer for distributed training\"\n",
    "        opt = self.accelerator.prepare_optimizer(self.learn.opt)\n",
    "        self.learn.opt = OptimWrapper(opt)\n",
    "        self.learn.accelerator._optimizers.append(self.learn.opt)\n",
    "        \n",
    "    def prepare_dataloader(self):\n",
    "        \"Prepares the active dl for distributed training\"\n",
    "        if self.accelerator.num_processes > 1:\n",
    "            self.learn.dl = DistributedDL(\n",
    "                self.learn.dl,\n",
    "                rank=self.accelerator.process_index,\n",
    "                world_size=self.accelerator.num_processes\n",
    "            )\n",
    "    \n",
    "    def before_train(self): self.prepare_dataloader()\n",
    "    \n",
    "    def before_validate(self): self.prepare_dataloader()\n",
    "    \n",
    "    def before_backward(self):\n",
    "        \"Call accelerator.backward\"\n",
    "        self.accelerator.backward(self.learn.loss_grad)\n",
    "        raise CancelBackwardException()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c32cd-bb63-4129-ba11-6955ff07996e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b45b32-8ab1-44c7-9ae4-c145641df0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
