{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e27b6-18ae-4e3d-8a83-30d5bb051997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed51fb4-473f-4c9b-a7d6-3474d3bcb31b",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Notebook containing patches to the main fastai library to make it compatible with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a101cfed-5100-4799-99cd-d97eb0b86a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from fastcore.basics import patch\n",
    "from fastai.metrics import AccumMetric, ActivationType\n",
    "from fastai.optimizer import Optimizer, OptimWrapper, _convert_params, pytorch_hp_map, _update\n",
    "from fastai.torch_core import to_device, default_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3d91d-d02f-4156-a7ae-dbda273f06ed",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "This is an optional section, but it is common practice in how the Accelerate examples are setup. In an ideal state, we can call `self.gather(*x)` which will then call `self.learn.accelerator.gather(*x)` to grab all of the distributed objects. This is extremely beneficial for calculating the metrics, and if we wanted to report a single loss, a similar method should be used in `_do_one_batch`, or at some point before Recorder writes `_valid_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4633efb0-9a00-4340-aeff-31d36239e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def accumulate(self:AccumMetric, learn):\n",
    "    \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "    pred = learn.pred\n",
    "    if hasattr(learn, \"accelerator\"):\n",
    "        pred, learn.y = learn.gather((pred, learn.y))\n",
    "    if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:\n",
    "        pred = F.softmax(pred, dim=self.dim_argmax)\n",
    "        if self.activation == ActivationType.BinarySoftmax: pred = pred[:, -1]\n",
    "    elif self.activation == ActivationType.Sigmoid: pred = torch.sigmoid(pred)\n",
    "    elif self.dim_argmax: pred = pred.argmax(dim=self.dim_argmax)\n",
    "    if self.thresh:  pred = (pred >= self.thresh)\n",
    "    self.accum_values(pred,learn.y,learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afebe32-b8cb-432d-9aae-f9308e58e282",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "This section adds two small patches to existing optimizer functions, `Optimizer.step` and `OptimWrapper.__init__`.\n",
    "\n",
    "The first was needed as Accelerate expects to be able to pass in a `closure` argument for some Optimizers. @ilovescience is eventually working on bringing in closure support to fastai, so this will be solved once that's done.\n",
    "\n",
    "The second is a small change to `OptimWrapper` to allow it to accept an already existing torch optimizer. This is so we can perform `accelerator.prepare()` on an existing optimizer and then bring it back into the state fastai expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47fb807e-c8fb-4da3-ad8a-a8c58c0796bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def step(self:Optimizer, closure=None):\n",
    "    for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "        for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))\n",
    "        self.state[p] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8046b48b-7938-468d-b731-2385f57d47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def __init__(self:OptimWrapper, params, opt, hp_map=None, convert_groups=True, **kwargs):\n",
    "    if callable(opt):\n",
    "        self.opt = opt(_convert_params(params), **kwargs) if convert_groups else opt(params, **kwargs)\n",
    "    else:\n",
    "        self.opt = opt\n",
    "    if hp_map is None: hp_map = pytorch_hp_map\n",
    "    self.fwd_map = {k: hp_map[k] if k in hp_map else k for k in detuplify_pg(self.opt.param_groups[0]).keys()}\n",
    "    self.bwd_map = {v:k for k,v in self.fwd_map.items()}\n",
    "    self.state = defaultdict(dict, {})\n",
    "    self.frozen_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67af9a3-4158-438b-a758-a6898ea4c820",
   "metadata": {},
   "source": [
    "## Learner\n",
    "\n",
    "Finally, we have adjustments to `Learner`. There is a change in the inner training loop (`_do_one_batch`) to use `accelerator.backward()` for propagation, a new `gather` function that can be helpful when trying to gather tensors across devices, and a new `_set_device` implementation that uses `accelerator.device` if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d34ec1d0-8e4a-4e9e-b3f2-67e3ed0f8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def gather(self:Learner, *items):\n",
    "    \"Gathers a tensor or list of tensors across all devices\"\n",
    "    return self.acelerator.gather(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c66a020-eefd-45cb-b7bb-8e544f3bbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _set_device(self:Learner, b):\n",
    "    if hasattr(self, \"accelerator\"):\n",
    "        return to_device(b, self.accelerator.device)\n",
    "    else:\n",
    "        model_device = torch.device(torch.cuda.current_device()) if next(self.model.parameters()).is_cuda else torch.device('cpu')\n",
    "        dls_device = getattr(self.dls, 'device', default_device())\n",
    "        if model_device == dls_device: return to_device(b, dls_device)\n",
    "        else: return to_device(b, model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fa09fa4-6f01-4aa5-8dd4-72e29bbb92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _do_one_batch(self:Learner):\n",
    "    self.pred = self.model(*self.xb)\n",
    "    self('after_pred')\n",
    "    if len(self.yb):\n",
    "        self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "    self('after_loss')\n",
    "    if not self.training or not len(self.yb): return\n",
    "    self('before_backward')\n",
    "    if hasattr(self, 'accelerator'):\n",
    "        self.accelerator.backward(self.loss_grad)\n",
    "    else:\n",
    "        self.loss_grad.backward()\n",
    "    self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "    self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0f0feb6-c36e-497b-a909-c9bd4c0bc5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script(\"00_core.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb773b2-e9da-462c-89cf-2f37f233d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
